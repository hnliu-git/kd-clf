
# Data module configs
data:
  batch_size: 64
  tokenizer: bhadresh-savani/distilbert-base-uncased-sentiment-sst2

# Distillation configs
distillation:
  learning_rate: 1e-5
#  freeze_teacher: true

# Weight-and-bias config
wandb:
  project: KD-exps
  exp: test_kd

# Setting pre-trained teacher
teacher_model:
  load_from_ckpt: false
  path: bhadresh-savani/distilbert-base-uncased-sentiment-sst2

# Custom student randomly initialized
student_model:
  hidden_size: 64
  hidden_layers: 2
  atten_heads: 4

# Pre-trained student
#student_model: prajjwal1/bert-tiny




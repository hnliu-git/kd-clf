
# Data module configs
data:
  batch_size: 32
  tokenizer: google/bert_uncased_L-2_H-512_A-8

# Distillation configs
distillation:
  learning_rate: 3e-5

# Weight-and-bias config
wandb:
  project: knowledge-distillation
  exp: base2tiny

# Setting pre-trained teacher
teacher_model: ckpts/bert-base-uncased-epoch=00-val_loss=0.21

# Custom student randomly initialized
student_model:
  hidden_size: 512
  hidden_layers: 2
  atten_heads: 8

# Pre-trained student
#student_model: google/bert_uncased_L-2_H-512_A-8




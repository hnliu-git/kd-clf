
# Data module configs
data:
  batch_size: 64
  num_workers: 4
  mlm_prob: 0.15
  max_seq_length: 256
  data_dir: /content/drive/MyDrive/thesis/cache/wikipedia_processed
  cache_dir: /content/drive/MyDrive/thesis/cache
  load_data_from_disk: 1
  tokenizer: bert-base-uncased


# Weight-and-bias config
wandb:
  project: pretrain
  exp: test

# The model
hidden_size: 144
hidden_layers: 2
atten_heads: 12
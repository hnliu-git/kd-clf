
# Data module configs
data:
  batch_size: 32
  tokenizer: bert-base-uncased
  dataset_name: 'tweet' # options ['sst2', 'tweet']

# Distillation configs
training:
  learning_rate: 2e-5

# Weight-and-bias config
wandb:
  project: finetune
  exp: bert-base-uncased-tweet

# Setting pre-trained model
model: bert-base-uncased
ckpt_path: ckpts/



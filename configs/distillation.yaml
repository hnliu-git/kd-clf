
# Data module configs
data:
  batch_size: 32
  val_dataset: 'sst2' # options ['sst2', 'tweet']
  trn_dataset: 'sst2-tweet' # options ['sst2', 'tweet', 'sst2-tweet']
  tokenizer: google/bert_uncased_L-2_H-128_A-2
  train_with_label: 0

# Distillation configs

distillation:
  learning_rate: 3e-5
  loss_list: ['pred:nll', 'pred:mse']

# Weight-and-bias config
wandb:
  project: knowledge-distillation-tweet
  exp: bert2tiny

# Setting pre-trained teacher
teacher_model: ckpts/bert-base-uncased-epoch=01-val_loss=0.61
#teacher_model: 'bert-base-uncased'

# Custom student randomly initialized
#student_model:
#  hidden_size: 144
#  hidden_layers: 2
#  atten_heads: 12

# Pre-trained student
student_model: google/bert_uncased_L-2_H-128_A-2



